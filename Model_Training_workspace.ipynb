{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset THIS IS THE OLD ONE WORK ON THE OTHER ONE\n",
    "\n",
    "This part of the code is essentially preparing the data for a machine learning model, transforming the text data into numerical form, and splitting the data into training and testing sets.\n",
    "\n",
    "The combined dataset is loaded from a CSV file using pandas' read_csv function and all news headlines for each record (each day) are concatenated into a single string. A CountVectorizer is initialized to convert the headlines into a matrix of token counts. The maximum number of features is set to 5000, but this can be adjusted based on computational capacity. A LabelEncoder is used to prepare the output matrix (Y) by transforming the labels into normalized encoding.\n",
    "\n",
    "The dataset is split into training and testing sets based on specific date ranges. The variables X_train, X_test, Y_train, and Y_test are defined in a later cell, which split the X and Y matrices into training and testing sets based on the indices of the original train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Georgia 'downs two Russian warplanes' as countries move to brink of war\" b'BREAKING: Musharraf to be impeached.' b'Russia Today: Columns of troops roll into South Ossetia; footage from fighting (YouTube)' b'Russian tanks are moving towards the capital of South Ossetia, which has reportedly been completely destroyed by Georgian artillery fire' b\"Afghan children raped with 'impunity,' U.N. official says - this is sick, a three year old was raped and they do nothing\" b'150 Russian tanks have entered South Ossetia whilst Georgia shoots down two Russian jets.' b\"Breaking: Georgia invades South Ossetia, Russia warned it would intervene on SO's side\" b\"The 'enemy combatent' trials are nothing but a sham: Salim Haman has been sentenced to 5 1/2 years, but will be kept longer anyway just because they feel like it.\" b'Georgian troops retreat from S. Osettain capital, presumably leaving several hundred people killed. [VIDEO]' b'Did the U.S. Prep Georgia for War with Russia?' b'Rice Gives Green Light for Israel to Attack Iran: Says U.S. has no veto over Israeli military ops' b'Announcing:Class Action Lawsuit on Behalf of American Public Against the FBI' b\"So---Russia and Georgia are at war and the NYT's top story is opening ceremonies of the Olympics?  What a fucking disgrace and yet further proof of the decline of journalism.\" b\"China tells Bush to stay out of other countries' affairs\" b'Did World War III start today?' b'Georgia Invades South Ossetia - if Russia gets involved, will NATO absorb Georgia and unleash a full scale war?' b'Al-Qaeda Faces Islamist Backlash' b'Condoleezza Rice: \"The US would not act to prevent an Israeli strike on Iran.\" Israeli Defense Minister Ehud Barak: \"Israel is prepared for uncompromising victory in the case of military hostilities.\"' b'This is a busy day:  The European Union has approved new sanctions against Iran in protest at its nuclear programme.' b\"Georgia will withdraw 1,000 soldiers from Iraq to help fight off Russian forces in Georgia's breakaway region of South Ossetia\" b'Why the Pentagon Thinks Attacking Iran is a Bad Idea - US News &amp; World Report' b'Caucasus in crisis: Georgia invades South Ossetia' b'Indian shoe manufactory  - And again in a series of \"you do not like your work?\"' b'Visitors Suffering from Mental Illnesses Banned from Olympics' b\"No Help for Mexico's Kidnapping Surge\"\n",
      "Shape of X: (1989, 5000)\n",
      "         Date                                      All_Headlines  Label\n",
      "0  2008-08-08  b\"Georgia 'downs two Russian warplanes' as cou...      0\n",
      "1  2008-08-11  b'Why wont America and Nato help us? If they w...      1\n",
      "2  2008-08-12  b'Remember that adorable 9-year-old who sang a...      0\n",
      "3  2008-08-13  b' U.S. refuses Israel weapons to attack Iran:...      0\n",
      "4  2008-08-14  b'All the experts admit that we should legalis...      1\n",
      "Shape of train data: (1611, 28)\n",
      "Shape of test data: (378, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "\n",
    "# Concatenate all news headlines into a single string for each record more efficiently\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Specify the date\n",
    "date = '2008-08-08'  # replace with the date you are interested in\n",
    "\n",
    "# Filter the dataframe and print the 'All_Headlines' column\n",
    "print(data[data['Date'] == date]['All_Headlines'].values[0])\n",
    "\n",
    "\n",
    "# Initialize a CountVectorizer with efficient memory usage\n",
    "vectorizer = CountVectorizer(max_features=5000, dtype='uint8')  # Using uint8 for memory efficiency\n",
    "X = vectorizer.fit_transform(data['All_Headlines'])\n",
    "\n",
    "# Display the shape of X\n",
    "print(\"Shape of X:\", X.shape)\n",
    "\n",
    "# Prepare the output matrix with LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Display the first few processed records to check everything went as expected\n",
    "print(data[['Date', 'All_Headlines', 'Label']].head())\n",
    "\n",
    "# Split the dataset into training and testing sets based on the provided date ranges\n",
    "train = data[(data['Date'] >= '2008-08-08') & (data['Date'] <= '2014-12-31')]\n",
    "test = data[(data['Date'] >= '2015-01-02') & (data['Date'] <= '2016-07-01')]\n",
    "\n",
    "# Print the shape of train and test data\n",
    "print(\"Shape of train data:\", train.shape)\n",
    "print(\"Shape of test data:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[data.index.isin(train.index)], X[data.index.isin(test.index)]\n",
    "Y_train, Y_test = Y[data.index.isin(train.index)], Y[data.index.isin(test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on the training set\n",
    "scaler.fit(X_train_dense)\n",
    "\n",
    "# Transform both the training set and the test set\n",
    "X_train_std = scaler.transform(X_train_dense)\n",
    "X_test_std = scaler.transform(X_test_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5179 - loss: 0.6964 - val_accuracy: 0.5603 - val_loss: 0.6845\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6016 - loss: 0.6660 - val_accuracy: 0.5327 - val_loss: 0.6903\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6820 - loss: 0.6384 - val_accuracy: 0.5050 - val_loss: 0.6928\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7515 - loss: 0.5882 - val_accuracy: 0.5276 - val_loss: 0.6941\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7927 - loss: 0.5266 - val_accuracy: 0.4874 - val_loss: 0.7204\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8562 - loss: 0.4436 - val_accuracy: 0.4824 - val_loss: 0.7445\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8993 - loss: 0.3540 - val_accuracy: 0.4849 - val_loss: 0.8049\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9254 - loss: 0.2723 - val_accuracy: 0.4749 - val_loss: 0.8746\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9604 - loss: 0.1967 - val_accuracy: 0.4874 - val_loss: 0.9643\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9797 - loss: 0.1376 - val_accuracy: 0.4799 - val_loss: 1.0879\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829us/step - accuracy: 0.4884 - loss: 1.0839\n",
      "Test Loss: 1.0885182619094849, Test Accuracy: 0.47989949584007263\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess dataset\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(data['All_Headlines']).toarray()\n",
    "Y = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Building the baseline model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, Y_train, validation_data=(X_test_scaled, Y_test), epochs=10, batch_size=256)\n",
    "\n",
    "# Evaluation\n",
    "evaluation = model.evaluate(X_test_scaled, Y_test)\n",
    "print(f'Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}')\n",
    "\n",
    "\n",
    "\n",
    "# # Enhancing the model - Adding more layers and neurons\n",
    "# enhanced_model = Sequential([\n",
    "#     Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# enhanced_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the enhanced model\n",
    "# enhanced_history = enhanced_model.fit(X_train_scaled, Y_train, validation_data=(X_test_scaled, Y_test), epochs=20, batch_size=128)\n",
    "\n",
    "# # Evaluation of the enhanced model\n",
    "# enhanced_evaluation = enhanced_model.evaluate(X_test_scaled, Y_test)\n",
    "# print(f'Enhanced Model - Test Loss: {enhanced_evaluation[0]}, Test Accuracy: {enhanced_evaluation[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
