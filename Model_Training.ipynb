{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset THIS IS THE OLD ONE WORK ON THE OTHER ONE\n",
    "\n",
    "This part of the code is essentially preparing the data for a machine learning model, transforming the text data into numerical form, and splitting the data into training and testing sets.\n",
    "\n",
    "The combined dataset is loaded from a CSV file using pandas' read_csv function and all news headlines for each record (each day) are concatenated into a single string. A CountVectorizer is initialized to convert the headlines into a matrix of token counts. The maximum number of features is set to 5000, but this can be adjusted based on computational capacity. A LabelEncoder is used to prepare the output matrix (Y) by transforming the labels into normalized encoding.\n",
    "\n",
    "The dataset is split into training and testing sets based on specific date ranges. The variables X_train, X_test, Y_train, and Y_test are defined in a later cell, which split the X and Y matrices into training and testing sets based on the indices of the original train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1989, 5000)\n",
      "         Date                                      All_Headlines  Label\n",
      "0  2008-08-08  b\"Georgia 'downs two Russian warplanes' as cou...      0\n",
      "1  2008-08-11  b'Why wont America and Nato help us? If they w...      1\n",
      "2  2008-08-12  b'Remember that adorable 9-year-old who sang a...      0\n",
      "3  2008-08-13  b' U.S. refuses Israel weapons to attack Iran:...      0\n",
      "4  2008-08-14  b'All the experts admit that we should legalis...      1\n",
      "Shape of train data: (1611, 28)\n",
      "Shape of test data: (378, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "\n",
    "# Concatenate all news headlines into a single string for each record more efficiently\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Initialize a CountVectorizer with efficient memory usage\n",
    "vectorizer = CountVectorizer(max_features=5000, dtype='uint8')  # Using uint8 for memory efficiency\n",
    "X = vectorizer.fit_transform(data['All_Headlines'])\n",
    "\n",
    "# Display the shape of X\n",
    "print(\"Shape of X:\", X.shape)\n",
    "\n",
    "# Prepare the output matrix with LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Display the first few processed records to check everything went as expected\n",
    "print(data[['Date', 'All_Headlines', 'Label']].head())\n",
    "\n",
    "# Split the dataset into training and testing sets based on the provided date ranges\n",
    "train = data[(data['Date'] >= '2008-08-08') & (data['Date'] <= '2014-12-31')]\n",
    "test = data[(data['Date'] >= '2015-01-02') & (data['Date'] <= '2016-07-01')]\n",
    "\n",
    "# Print the shape of train and test data\n",
    "print(\"Shape of train data:\", train.shape)\n",
    "print(\"Shape of test data:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[data.index.isin(train.index)], X[data.index.isin(test.index)]\n",
    "Y_train, Y_test = Y[data.index.isin(train.index)], Y[data.index.isin(test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on the training set\n",
    "scaler.fit(X_train_dense)\n",
    "\n",
    "# Transform both the training set and the test set\n",
    "X_train_std = scaler.transform(X_train_dense)\n",
    "X_test_std = scaler.transform(X_test_dense)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
