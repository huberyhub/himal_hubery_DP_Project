{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset\n",
    "\n",
    "This part of the code is essentially preparing the data for a machine learning model, transforming the text data into numerical form, and splitting the data into training and testing sets.\n",
    "\n",
    "The combined dataset is loaded from a CSV file using pandas' read_csv function and all news headlines for each record (each day) are concatenated into a single string. A CountVectorizer is initialized to convert the headlines into a matrix of token counts. The maximum number of features is set to 5000, but this can be adjusted based on computational capacity. A LabelEncoder is used to prepare the output matrix (Y) by transforming the labels into normalized encoding.\n",
    "\n",
    "The dataset is split into training and testing sets based on specific date ranges. The variables X_train, X_test, Y_train, and Y_test are defined in a later cell, which split the X and Y matrices into training and testing sets based on the indices of the original train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1989, 5000)\n",
      "Shape of Y: (1989,)\n",
      "         Date                                      All_Headlines  Label\n",
      "0  2008-08-08  b\"Georgia 'downs two Russian warplanes' as cou...      0\n",
      "1  2008-08-11  b'Why wont America and Nato help us? If they w...      1\n",
      "2  2008-08-12  b'Remember that adorable 9-year-old who sang a...      0\n",
      "3  2008-08-13  b' U.S. refuses Israel weapons to attack Iran:...      0\n",
      "4  2008-08-14  b'All the experts admit that we should legalis...      1\n"
     ]
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "\n",
    "# Concatenate all the news headlines into a single string for each record\n",
    "# Optimized by directly using pandas functionality\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Initialize a CountVectorizer to convert the headlines to a matrix of token counts\n",
    "# Consider limiting max_features and experimenting with ngram_range for better performance\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(data['All_Headlines'])\n",
    "\n",
    "# Prepare the output matrix\n",
    "Y = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Verify the shapes of the matrices and the first few rows to ensure the preprocessing is as expected.\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "print(data[['Date', 'All_Headlines', 'Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Typically, you might want to use 80% of the data for training and 20% for testing, but these proportions can be adjusted.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(X, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Standardizing the features: since X is a sparse matrix returned by CountVectorizer, \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# we use MaxAbsScaler which is more appropriate for sparse data. StandardScaler is generally not used for sparse data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# because it can break the sparsity structure.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MaxAbsScaler()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "# Typically, you might want to use 80% of the data for training and 20% for testing, but these proportions can be adjusted.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features: since X is a sparse matrix returned by CountVectorizer, \n",
    "# we use MaxAbsScaler which is more appropriate for sparse data. StandardScaler is generally not used for sparse data\n",
    "# because it can break the sparsity structure.\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verify the standardization and splitting by printing shapes\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Baseline Model\n",
    "Below is an example extension adding a simple logistic regression model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.46733668341708545\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.43      0.41       171\n",
      "           1       0.54      0.50      0.52       227\n",
      "\n",
      "    accuracy                           0.47       398\n",
      "   macro avg       0.46      0.46      0.46       398\n",
      "weighted avg       0.47      0.47      0.47       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model on the scaled training data\n",
    "lr_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Predict on the scaled testing data\n",
    "Y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy on Test Set:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat-GPT Based Model\n",
    "\n",
    "We will use a Chat-GPT Based Model for predicting stock market trends from news headlines. We will use PyTorch and the Hugging Face Transformers library. Given the project's nature, we'll focus on using a pre-trained GPT model and fine-tuning it to the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess the Dataset\n",
    "\n",
    "We need to preprocess it into a format suitable for a GPT model. This involves tokenization and encoding the headlines, as well as preparing the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        headline = str(self.headlines[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            headline,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'headline_text': headline,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loaders\n",
    "\n",
    "Preparing the dataset for training and validation. This step involves creating DataLoader instances for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Function to create data loaders\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = NewsDataset(\n",
    "        headlines=df['All_Headlines'].to_numpy(),\n",
    "        labels=df['Label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=0) # Using num_workers=0 for compatibility\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token # Setting pad token\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "model.config.pad_token_id = model.config.eos_token_id # Ensure the model accepts pad_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Data loaders\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we will define the model training and evaluation loop. This involves loading the GPT-2 model for sequence classification, defining the optimizer, and iterating over the dataset to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        labels = d['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [04:11<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.8798575592041016 accuracy 0.5084852294154619\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [04:01<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6907531183958053 accuracy 0.5474544311753614\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [03:36<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.6525920072197914 accuracy 0.6096794468887492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(train_df)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [04:08<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5994312298297882 accuracy 0.6744186046511628\n",
      "Epoch 2/6\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [03:43<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5150315748900175 accuracy 0.7573852922690132\n",
      "Epoch 3/6\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [03:57<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.37972548872232437 accuracy 0.8372093023255814\n",
      "Epoch 4/6\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [03:58<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.26156691592186687 accuracy 0.8950345694531741\n",
      "Epoch 5/6\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [03:43<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.19294259852729737 accuracy 0.9340037712130735\n",
      "Epoch 6/6\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [03:42<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.1268861471489072 accuracy 0.9566310496543055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 6\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(train_df)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Japan struck by 6.7 magnitude earthquake. After shocks expected. President Barack Obama:''Palestinians deserve an end to occupation'' Vatican officially recognizes 'state of Palestine' in new treaty \"Time has come to reexamine cannabis prohibition, Israel's police chief says\" Developing reports claim that Pakistan's army not only knew where Osama bin Laden was hiding, but complicit in protecting him since 2006 Tory officials threatened BBC during election - senior BBC executives faced repeated threats of far-reaching reforms if they didnt change election campaign coverage Pope: God will judge you on whether you cared for Earth BBC reporting a coup in Burundi has effectively removed President Nkurunziza. 40 shia muslims gunned down by gunmen in a bus in Pakistan. Chancellor Angela Merkel is set to face further political embarrassment over Berlins spy scandal following new revelations that US intelligence planned to obtain unlimited access to Germanys main internet cable networks in its attempt to achieve saturation surveillance. A district of 5 million people in Chinas restive far west has demanded that residents hand in their passports to the police for indefinite safekeeping, the latest government crackdown in an area where Beijing has declared a peoples war on violent separatists. David Cameron is to set out a string of new powers to tackle radicalisation, saying the UK has been a \"passively tolerant society\" for too long | The bill will include new immigration rules, powers to close down premises used by extremists and \"extremism disruption orders\". More than 5,000 people have signed a petition calling for the north of England to secede from the UK and join Scotland Sea Level Rise Is Happening Faster Than Anyone Thought - The study, published Monday in Nature Climate Change, found that sea level rise has been speeding up over the past two decades compared to the rest of the 20th century. This contradicts previous satellite data dating back to 1993 Letting Shell drill in Arctic could lead to catastrophic oil spill, experts warn | Lack of local infrastructure to undertake oil exploration will prove devastating, environmentalists say  there is one road, no rail system and limited air facilities Isis Jihadis using Passports Stolen from Westerners to Travel to Syria. An Illinois resident, claimed that she along with her husband was visiting Paris, when their passports were stolen. Her passport was found from an Isis safehouse in Syria New York Times correspondent backs up Sy Hersh claim about how the US really found bin Laden Former European leaders call for change in EU policy on Israel | Europe must hold Israel to account for the way it maintains the occupation, says letter from former prime ministers and diplomats Dawn Spacecraft Images Reveal \"Ice Rinks\" on Ceres - The presence of exposed ice on Ceres would be a surprise, because ice should be unstable on the dwarf planets airless surface, turning from a solid into a gas and drifting into space rather than sticking around. Iran summarily executed 98 people in a month: U.N. report Greece taps out its emergency IMF reserves  to pay back IMF Malaysia says it will turn away migrants stranded at sea unless boats are sinking Rights group: Human rights under Palestinian rule worsened 10 Chinese Arrested For Killing Giant Panda, Selling Parts Strong 6.8-magnitude earthquake hits northeastern Japan\n",
      "Predicted sentiment: Decrease\n",
      "Actual movement: Decrease\n",
      "---\n",
      "Headline: Castro resigns as head of Cuban Communist party, allows Cubans to buy and sell property for first time since 1959 Secret memos expose link between oil firms and invasion of Iraq. Plans to exploit Iraq's oil reserves were discussed by government ministers and the world's largest oil companies the year before Britain took a leading role in invading Iraq. 70 days jail for burning Koran in UK \n",
      "'I will never be cut': Kenyan girls fight back against genital mutilation - video Iceland Declares Independence from International Banks China is scary as hell Death toll in Libya reaches 10,000 Hungary Is a Disgrace for Europe  Banks can't spell \"Gadhafi\" either, so they can't freeze his assets.  Carl-Henric Svanberg, President of BP and known patron of \"the small people\", does it again: Claims \"Most would agree that the world can withstand a few degrees of warming\" in Swedish interview Israeli Intellectuals Press for Palestinian State - \n",
      "NYTimes.com Fidel quits Communist Party leadership as Cuba looks to reform More Bombs in Northern Ireland Japanese Ministry of Internal Affairs creates project team to combat rumors deemed harmful to Japanese security in the wake of Fukushima. They begun sending \"letters of request\" to internet providers and others demanding that they take adequate measures\" in response to \"illegal information\". CEO of ThyssenKrupp in Italy sentenced to 16.5 years \n",
      "for the deaths of seven workers after a fire Bahrain escapes censure by West as crackdown on protesters intensifies - Middle East - Independent.co.uk Bulgarian nationalist attacked Jehovah witnesses meeting [violent video]  N.Korea's Nuclear Facilities 'a Disaster in the Making' UN Report - A copy of the report obtained by The Australian found credible allegations that tens of thousands of Tamil civilians were killed deliberately targeted by Sri Lankan artillery firing into \"no-fire zones\".| The Australian How could bodies pile up again in a place just 90 miles from the U.S. border where 72 migrants were slaughtered eight months ago...And how could these new horrors emerge only five months after state and federal authorities announced with much fanfare that they were mounting a coordinated offensive Japan consumers may bail out nuke plant owner Australia deports criminal to UK. French minimum wage to rise again\n",
      " Estonia Tops List for Internet Freedom Kurdish protesters clashed with police in Turkeys southeast and thousands gathered in Istanbul and Ankara after 12 pro-Kurdish politicians were banned from running in elections\n",
      "Predicted sentiment: Decrease\n",
      "Actual movement: Increase\n",
      "---\n",
      "Headline: Moroccans demand change to Islamic penal code after a girl kills herself because judge forced her to marry her rapist Man called police after finding pornographic images on his laptop after trying to download music. Social services banned him from having unsupervised access to his child while police investigated... Ban was finally lifted The Pirate Party, which made international headlines after its success in state elections in Berlin last year, stands to make gains in two other German states this spring. The party's pledge to foster transparency and participation is resonating with voters who are fed up with local corruption.  Lost Boy Uses Google Earth to Find His Way Home After 25 Years Colombia to Enact Drug Decriminalization: The govt of Colombian President Juan Manuel Santos is preparing legislation that will set \"personal dose\" amounts for drugs that will allow for their possession without the possibility of arrest or prosecution. The Guardian obtained 3000 messages from al-Asaad's email account.  Iran advised him on how to handle the uprising against his rule. And Qatar offered him exile  Kony 2012 video screening met with anger in northern Uganda Officials in China's Ministry of Health recently admitted to harvesting organs from death row inmates, ending years of denials about the practice. It is estimated that two-thirds of the country's total organ transplants come from executed prisoners 50 NGOs urge Pakistan to free Christian mother  from execution on charge of Blasphemy --- \"We refuse to accept that this mother of five children will continue to languish in her putrid, stifling, freezing cell for so long  especially without the right to see her children.\" French police use tear gas on steelworkers worried about job losses, as they tried to force their way toward the headquarters of President Sarkozy's re-election campaign Syria setting border mines to stop refugee flight Ugandan Gay Rights Group Sues Scott Lively, a U.S. Evangelist - NYTimes.com Illegal Fishing Straining West Africa -  Recognized as one of the world's richest fisheries grounds teeming with snapper, grouper, sardines, mackerel &amp; shrimp, it's losing up to $1.5 billion worth of fish annually to vessels in protected zones or without proper equipment or licenses\n",
      " Iran loses access to SWIFT  Nato \"Must Quit Afghan Villages\" - President Karzai tells Nato to pull back to bases Suspect in Afghan massacre flown out of country, and will not face justice in Afghanistan. Norwegian Hercules plane vanishes en route to Sweden Afghans seethe at lack of US reaction to massacre UK Teen Charged Over Dead Soldiers Facebook Post: '... teenager will appear in court for allegedly making comments on Facebook about the deaths of six British soldiers in Afghanistan.' China Sacks Key Leadership Contender - One of Chinas most charismatic western-style politicians has been sacked in rare political scandal In a historic judgment, the International Criminal Court convicts Thomas Lubango Dyilo of recruiting children into armed conflict Bo Xilai: a stunning &amp; highly public fall from grace in China\n",
      " -\n",
      "The senior Communist Party official, was abruptly dismissed amid scandal, ending his ambition of a top post. His removal could complicate a key year of political transition in China.  Egypt: Armed Bedouins are besieging the UN peacekeeper camp in Sinai Court Convicts Congo Warlord Of Using Child Soldiers Canadian women 'molested by female US border guards,' lawsuit alleges\n",
      "Predicted sentiment: Decrease\n",
      "Actual movement: Increase\n",
      "---\n",
      "Headline: b'...and I am to Blame?' b'Italy bans kebabs and foreign food from cities' b'Israel has refused to allow a French-made water purification system into Gaza amid a drinking water crisis in the Palestinian strip' b'Palestinians are forbidden from crossing the road or traveling on the road by foot, animal or automobile' b'Israeli ambassador to Australia slips up and discloses possible action against Iran in a month' b'Leaked report on Jewish settlements in the West Bank shows that the Israeli government was complicit in illegal construction on land owned by Palestinians' b'France says Israel blocks water equipment for Gaza' b'Jews are not Zionists!' b'Were Europeans once cannibals?  - You bet!' b'Israel denies Gaza access to clean water' b\"Britain and Germany want International Financial Watchdog so America can't fuck up the world ... again.\" b\"Spain opens investigation of Israel under it's universal jurisdiction law for crimes against humanity in case of 2002 Israeli bombing in Gaza which killed 14 innocent civilians\" b'All Australians could be implanted with microchips for tracking and identification within the next two or three generations, a prominent academic says.' b'Can Countries Really Go Bankrupt?' b\"DEA quits Bolivia on Morales' order\" b\"Thousands protest across Russia demanding Putin's resignation\" b\"Mafia business 'equal' to 9% of Italian GDP\" b'\"The military should close its torture school. I know because I graduated from it.\"' b'We are on the brink of war with the South, says North Korea' b'Governments across Europe tremble as angry people take to the streets' b\"Fake pharmacies plague Iraq: There is no oversight to ensure drugs aren't counterfeit or contaminated.\" b'Israeli Ambassador to Australia Says Will \"Deal With\" Iran\\'s Nuclear Program Within a Month [vid]' b'Turkish PM hailed as a hero' b'Testimony of a man who trained child soldiers in DR Congo' b'Database implicates Israeli government in settlement land grab'\n",
      "Predicted sentiment: Decrease\n",
      "Actual movement: Decrease\n",
      "---\n",
      "Headline: American dentist named as the hunter that killed beloved Cecil the lion. Billionaire hedge fund managers have called on Puerto Rico to lay off teachers and close schools so that the island can pay them back the billions it owes. It accused the island, where 56% of children live in poverty, of spending too much on education U.S. drops bid for 2024 Olympics in Boston Porn websites visited 250,000 times on UK parliament computers Canadian Conservative MPs block their own Finance Minister from presenting a report about the Canadian economy. Prime Minister John Key has admitted that New Zealand will have to pay more for medicines if it signs up to the Trans Pacific Partnership but he says this was unlikely to affect consumers. DNA discovery: Modern Amazonians linked to indigenous Australians Killing of Cecil the lion prompts calls for EU ban on importing lion trophies: Zimbabwes famous lion was lured out of a national park, killed, beheaded and skinned. Over 200 lions are legally killed and turned into trophies and sent to Europe every year Jehovah's Witnesses did not report 1006 alleged sex abusers to police Apple, Microsoft, Google and other US firms to commit $140bn to address climate change Facing corruption scandal, Malaysian PM fires officials investigating him 950 million Android phones can be hijacked by malicious text messages Swiss officials have issued a formal apology after it emerged army helicopters had crossed the border with France on Thursday in an unexpected incursion to draw water for thirsty Swiss cows. Senior London Shia scholar declares jihad against extremism, against the forces misusing the name of Islam; against ISIS Anonymous releases hacked CSIS document after members death, threaten to leak stunning secrets Insulting statues of 'rat bankers' have appeared in London Members of an isolated indigenous group made contact over the weekend with villagers in Peru's Amazon basin seeking food and supplies Philip Morris sues Australian Government over plain packaging laws North Korean authorities going house-to-house in search to destroy 'banned' music CDs and tapes 40 Percent of the World's Adults Have Never Heard of Climate Change LGBT hate speech to be outlawed in Poland for the first time Obama condemns African leaders who won't give up power NATO holding rare emergency meeting at Turkey's request The UK government spent 13 times more bombing Libya than securing peace in the years afterwards - 450m versus 35m Libya: Moammar Gadhafi's son Saif al-Islam sentenced to death\n",
      "Predicted sentiment: Decrease\n",
      "Actual movement: Increase\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Assuming val_df is your validation DataFrame\n",
    "# Let's take a small sample for testing\n",
    "test_sample = val_df.sample(n=5).reset_index(drop=True)\n",
    "\n",
    "# Create a test DataLoader\n",
    "# Corrected function call - Ensure BATCH_SIZE is passed correctly as part of the function's arguments, not as a named parameter\n",
    "test_data_loader = create_data_loader(test_sample, tokenizer, MAX_LEN, 1)  # Use 1 for BATCH_SIZE here\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    \n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(labels)\n",
    "    \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return predictions, real_values\n",
    "\n",
    "# Take a small sample for testing\n",
    "test_sample = val_df.sample(n=5).reset_index(drop=True)\n",
    "\n",
    "# Corrected call to create_data_loader with the correct argument passing\n",
    "test_data_loader = create_data_loader(test_sample, tokenizer, MAX_LEN, 1)  # Correctly passing batch size\n",
    "\n",
    "# Evaluate the model on the test sample\n",
    "predictions, real_values = evaluate(model, test_data_loader, device)\n",
    "\n",
    "# Displaying the predictions and actual labels\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Headline: {test_sample['All_Headlines'].iloc[i]}\")\n",
    "    print(f\"Predicted sentiment: {'Increase' if predictions[i] == 1 else 'Decrease'}\")\n",
    "    print(f\"Actual movement: {'Increase' if real_values[i] == 1 else 'Decrease'}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Save the model weights\n",
    "model.save_pretrained('path_to_save_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised Final:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 26/100 [01:04<03:02,  2.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Function to evaluate the model on test data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 91\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, device, n_examples)\u001b[0m\n\u001b[1;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 91\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\n\u001b[1;32m     93\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a custom Dataset class for tokenization and batching\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        headline = str(self.headlines[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            headline,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'headline_text': headline,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = NewsDataset(\n",
    "        headlines=df['All_Headlines'].to_numpy(),\n",
    "        labels=df['Label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as pad token\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "model.config.pad_token_id = model.config.eos_token_id  # Configuring pad token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        labels = d['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        len(train_df)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "# Function to evaluate the model on test data\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(labels)\n",
    "    \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return predictions, real_values\n",
    "\n",
    "# Testing the model with a small sample from the validation set\n",
    "test_sample = val_df.sample(n=5).reset_index(drop=True)\n",
    "test_data_loader = create_data_loader(test_sample, tokenizer, MAX_LEN, 1)  # Single batch size for individual assessment\n",
    "\n",
    "predictions, real_values = evaluate(model, test_data_loader, device)\n",
    "\n",
    "# Print predictions and actual labels\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Headline: {test_sample['All_Headlines'].iloc[i]}\")\n",
    "    print(f\"Predicted sentiment: {'Increase' if predictions[i] == 1 else 'Decrease'}\")\n",
    "    print(f\"Actual movement: {'Increase' if real_values[i] == 1 else 'Decrease'}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
