{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset\n",
    "\n",
    "This part of the code is essentially preparing the data for a machine learning model, transforming the text data into numerical form, and splitting the data into training and testing sets.\n",
    "\n",
    "The combined dataset is loaded from a CSV file using pandas' read_csv function and all news headlines for each record (each day) are concatenated into a single string. A CountVectorizer is initialized to convert the headlines into a matrix of token counts. The maximum number of features is set to 5000, but this can be adjusted based on computational capacity. A LabelEncoder is used to prepare the output matrix (Y) by transforming the labels into normalized encoding.\n",
    "\n",
    "The dataset is split into training and testing sets based on specific date ranges. The variables X_train, X_test, Y_train, and Y_test are defined in a later cell, which split the X and Y matrices into training and testing sets based on the indices of the original train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1989, 5000)\n",
      "Shape of Y: (1989,)\n",
      "         Date                                      All_Headlines  Label\n",
      "0  2008-08-08  b\"Georgia 'downs two Russian warplanes' as cou...      0\n",
      "1  2008-08-11  b'Why wont America and Nato help us? If they w...      1\n",
      "2  2008-08-12  b'Remember that adorable 9-year-old who sang a...      0\n",
      "3  2008-08-13  b' U.S. refuses Israel weapons to attack Iran:...      0\n",
      "4  2008-08-14  b'All the experts admit that we should legalis...      1\n"
     ]
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "data = np.squeeze(data)\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "\n",
    "# Concatenate all the news headlines into a single string for each record\n",
    "# Optimized by directly using pandas functionality\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Initialize a CountVectorizer to convert the headlines to a matrix of token counts\n",
    "# Consider limiting max_features and experimenting with ngram_range for better performance\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(data['All_Headlines'])\n",
    "\n",
    "# Prepare the output matrix\n",
    "Y = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Verify the shapes of the matrices and the first few rows to ensure the preprocessing is as expected.\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "print(data[['Date', 'All_Headlines', 'Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (1591, 5000)\n",
      "X_test_scaled shape: (398, 5000)\n",
      "Y_train shape: (1591,)\n",
      "Y_test shape: (398,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "# Typically, you might want to use 80% of the data for training and 20% for testing, but these proportions can be adjusted.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features: since X is a sparse matrix returned by CountVectorizer, \n",
    "# we use MaxAbsScaler which is more appropriate for sparse data. StandardScaler is generally not used for sparse data\n",
    "# because it can break the sparsity structure.\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verify the standardization and splitting by printing shapes\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Baseline Model\n",
    "Below is an example extension adding a simple logistic regression model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.46733668341708545\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.43      0.41       171\n",
      "           1       0.54      0.50      0.52       227\n",
      "\n",
      "    accuracy                           0.47       398\n",
      "   macro avg       0.46      0.46      0.46       398\n",
      "weighted avg       0.47      0.47      0.47       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model on the scaled training data\n",
    "lr_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Predict on the scaled testing data\n",
    "Y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy on Test Set:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat-GPT Based Model\n",
    "\n",
    "We will use a Chat-GPT Based Model for predicting stock market trends from news headlines. We will use PyTorch and the Hugging Face Transformers library. Given the project's nature, we'll focus on using a pre-trained GPT model and fine-tuning it to the dataset. \n",
    "\n",
    "## Load and Preprocess the Dataset\n",
    "\n",
    "We need to preprocess it into a format suitable for a GPT model. This involves tokenization and encoding the headlines, as well as preparing the labels.\n",
    "\n",
    "Hubery run this:\n",
    "pip install torch torchvision transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "# We need to preprocess it into a format suitable for a GPT model. \n",
    "# This involves tokenization and encoding the headlines, as well as preparing the labels.\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenizing dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        headline = str(self.headlines[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          headline,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "          'headline_text': headline,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loaders\n",
    "\n",
    "Preparing the dataset for training and validation. This step involves creating DataLoader instances for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = NewsDataset(\n",
    "        headlines=df.All_Headlines.to_numpy(),\n",
    "        labels=df.Label.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Assuming `data` DataFrame contains the 'All_Headlines' and 'Label' columns\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0217, 0.9783]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load GPT-2 For Sequence Classification\n",
    "# You might want to specify the number of labels depending on your task\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "\n",
    "# Example text\n",
    "text = \"This new AI model is fantastic!\"\n",
    "\n",
    "# Encode the text into tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Predict with the model\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Convert logits to probabilities (optional)\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we will define the model training and evaluation loop. This involves loading the GPT-2 model for sequence classification, defining the optimizer, and iterating over the dataset to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
