{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset\n",
    "\n",
    "This part of the code is essentially preparing the data for a machine learning model, transforming the text data into numerical form, and splitting the data into training and testing sets.\n",
    "\n",
    "The combined dataset is loaded from a CSV file using pandas' read_csv function and all news headlines for each record (each day) are concatenated into a single string. A CountVectorizer is initialized to convert the headlines into a matrix of token counts. The maximum number of features is set to 5000, but this can be adjusted based on computational capacity. A LabelEncoder is used to prepare the output matrix (Y) by transforming the labels into normalized encoding.\n",
    "\n",
    "The dataset is split into training and testing sets based on specific date ranges. The variables X_train, X_test, Y_train, and Y_test are defined in a later cell, which split the X and Y matrices into training and testing sets based on the indices of the original train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1989, 5000)\n",
      "Shape of Y: (1989,)\n",
      "         Date                                      All_Headlines  Label\n",
      "0  2008-08-08  b\"Georgia 'downs two Russian warplanes' as cou...      0\n",
      "1  2008-08-11  b'Why wont America and Nato help us? If they w...      1\n",
      "2  2008-08-12  b'Remember that adorable 9-year-old who sang a...      0\n",
      "3  2008-08-13  b' U.S. refuses Israel weapons to attack Iran:...      0\n",
      "4  2008-08-14  b'All the experts admit that we should legalis...      1\n"
     ]
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "data = pd.read_csv('Datasets/Combined_News_DJIA.csv')\n",
    "\n",
    "# Concatenate all the news headlines into a single string for each record\n",
    "# Optimized by directly using pandas functionality\n",
    "data['All_Headlines'] = data.iloc[:, 2:].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Initialize a CountVectorizer to convert the headlines to a matrix of token counts\n",
    "# Consider limiting max_features and experimenting with ngram_range for better performance\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
    "X = vectorizer.fit_transform(data['All_Headlines'])\n",
    "\n",
    "# Prepare the output matrix\n",
    "Y = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Verify the shapes of the matrices and the first few rows to ensure the preprocessing is as expected.\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "print(data[['Date', 'All_Headlines', 'Label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (1591, 5000)\n",
      "X_test_scaled shape: (398, 5000)\n",
      "Y_train shape: (1591,)\n",
      "Y_test shape: (398,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "# Typically, you might want to use 80% of the data for training and 20% for testing, but these proportions can be adjusted.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the features: since X is a sparse matrix returned by CountVectorizer, \n",
    "# we use MaxAbsScaler which is more appropriate for sparse data. StandardScaler is generally not used for sparse data\n",
    "# because it can break the sparsity structure.\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Verify the standardization and splitting by printing shapes\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Baseline Model\n",
    "Below is an example extension adding a simple logistic regression model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set: 0.46733668341708545\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.43      0.41       171\n",
      "           1       0.54      0.50      0.52       227\n",
      "\n",
      "    accuracy                           0.47       398\n",
      "   macro avg       0.46      0.46      0.46       398\n",
      "weighted avg       0.47      0.47      0.47       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model on the scaled training data\n",
    "lr_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# Predict on the scaled testing data\n",
    "Y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy on Test Set:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat-GPT Based Model\n",
    "\n",
    "We will use a Chat-GPT Based Model for predicting stock market trends from news headlines. We will use PyTorch and the Hugging Face Transformers library. Given the project's nature, we'll focus on using a pre-trained GPT model and fine-tuning it to the dataset. \n",
    "\n",
    "## Load and Preprocess the Dataset\n",
    "\n",
    "We need to preprocess it into a format suitable for a GPT model. This involves tokenization and encoding the headlines, as well as preparing the labels.\n",
    "\n",
    "Hubery run this:\n",
    "pip install torch torchvision transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "# We need to preprocess it into a format suitable for a GPT model. \n",
    "# This involves tokenization and encoding the headlines, as well as preparing the labels.\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Tokenizing dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        headline = str(self.headlines[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          headline,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "          'headline_text': headline,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loaders\n",
    "\n",
    "Preparing the dataset for training and validation. This step involves creating DataLoader instances for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = NewsDataset(\n",
    "        headlines=df.All_Headlines.to_numpy(),\n",
    "        labels=df.Label.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Assuming `data` DataFrame contains the 'All_Headlines' and 'Label' columns\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9990, 0.0010]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load GPT-2 For Sequence Classification\n",
    "# You might want to specify the number of labels depending on your task\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "\n",
    "# Example text\n",
    "text = \"This new AI model is fantastic!\"\n",
    "\n",
    "# Encode the text into tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Predict with the model\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Convert logits to probabilities (optional)\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Print the probabilities\n",
    "print(probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we will define the model training and evaluation loop. This involves loading the GPT-2 model for sequence classification, defining the optimizer, and iterating over the dataset to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(predictions == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'NewsDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m val_acc, val_loss \u001b[38;5;241m=\u001b[39m eval_model(\n\u001b[1;32m     53\u001b[0m     model,\n\u001b[1;32m     54\u001b[0m     val_data_loader,\n\u001b[1;32m     55\u001b[0m     device,\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mlen\u001b[39m(test_df)\n\u001b[1;32m     57\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     14\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Next steps\n",
    "\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Assuming optimizer is defined\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "epochs = 4\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train_df)\n",
    "    )\n",
    "    \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        device,\n",
    "        len(test_df)\n",
    "    )\n",
    "    \n",
    "    print(f'Validation loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc\n",
    "        \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, headlines, labels, tokenizer, max_len):\n",
    "        self.headlines = headlines\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.headlines)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        headline = str(self.headlines[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            headline,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'headline_text': headline,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Split\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = NewsDataset(\n",
    "        headlines=df['All_Headlines'].to_numpy(),\n",
    "        labels=df['Label'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(ds, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming train_df and val_df are defined somewhere above after loading your CSV\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Split\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2) # Assuming binary classification\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "#Split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in tqdm(data_loader):\n",
    "        input_ids = d['input_ids'].to(device)\n",
    "        attention_mask = d['attention_mask'].to(device)\n",
    "        labels = d['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
